{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #800080; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: left; font-size: 32px;\">Welcome to my notebook</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This Python 3 environment comes with many helpful analytics libraries installed\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# For example, here's several helpful packages to load\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;66;03m# linear algebra\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Input data files are available in the read-only \"../input/\" directory\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<div style=\"font-family: Cambria; font-weight: bold; letter-spacing: 0px; color: #ffffff; font-size: 120%; text-align: left; padding: 3px; background-color: #0000ff; border-bottom: 10px solid #80ffff;\">\n",
    "    Data inspection\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.079911,
     "end_time": "2024-04-22T18:45:27.153959",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.074048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/kaggle/input/food-choices/food_coded.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.037989,
     "end_time": "2024-04-22T18:45:27.201207",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.163218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.022731,
     "end_time": "2024-04-22T18:45:27.233709",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.210978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.024059,
     "end_time": "2024-04-22T18:45:27.267142",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.243083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.030374,
     "end_time": "2024-04-22T18:45:27.307019",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.276645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffff00; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: left; font-size: 20px;\">\n",
    "               Observation<br><br>\n",
    "        1.The dataset comprises 125 entries with 61 columns, containing various data types, including integers, floats, and objects (strings).<br>\n",
    "        2.Many columns have missing values, with the number of non-null entries ranging from 99 to 125.<br>\n",
    "        3.The 'Gender' column is binary, with values of 1 and 2, while other columns, such as 'GPA' and 'weight,' are stored as objects but should be numerical.<br>\n",
    "        4.Descriptive statistics show a wide range of values across the columns, indicating diverse data points.\n",
    "    </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Cambria; font-weight: bold; letter-spacing: 0px; color: #ffffff; font-size: 120%; text-align: left; padding: 3px; background-color: #0000ff; border-bottom: 10px solid #80ffff;\">\n",
    "    Data cleaning\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #ff0000; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: left; font-size: 15px;\"> Handling missing data </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008000; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: left; font-size: 32px; color: white;\">Handling Missing Values</h1>\n",
    "    <p style=\"color: white; font-size: 18px;\">\n",
    "        <b>Methods for Handling Missing Values</b>\n",
    "        <br><br>\n",
    "        <b>1.Complete Case Analysis (CCA):</b>\n",
    "        <br>\n",
    "        In complete case analysis, also known as listwise deletion, you simply remove any observations from your dataset that contain missing values in any of the columns. \n",
    "        This method is straightforward and easy to implement. However, it can lead to loss of valuable information, especially if the missing values are not missing completely at random (MCAR).\n",
    "        <br><br>\n",
    "        <b>2.Pairwise Deletion:</b>\n",
    "        <br>\n",
    "        With pairwise deletion, also known as available case analysis, you retain observations with missing values for specific analyses but only use the available data for each calculation. \n",
    "        This method allows you to maximize the use of available data without discarding entire observations. However, it can lead to biased estimates if missingness is related to the variables being analyzed.\n",
    "        <br><br>\n",
    "        <b>3.Threshold-based Removal:</b>\n",
    "        <br>\n",
    "        In threshold-based removal, you set a threshold for the proportion of missing values allowed in each observation or column. Observations or columns exceeding this threshold are removed from the dataset. \n",
    "        This method allows you to control the extent of data loss by adjusting the threshold. However, choosing an appropriate threshold can be subjective and may require domain knowledge or sensitivity analysis.\n",
    "        <br><br>\n",
    "        <b>4.Column-wise Removal:</b>\n",
    "        <br>\n",
    "        If certain columns in your dataset have a high proportion of missing values and are not critical for your analysis, you may choose to remove those columns entirely. \n",
    "        This method can help reduce the complexity of your dataset and focus your analysis on the most informative variables. However, it may result in the loss of potentially relevant information, so it's essential to carefully evaluate the importance of each column.\n",
    "        <br><br>\n",
    "        <b>5.Row-wise Removal:</b>\n",
    "        <br>\n",
    "        Similarly, if certain observations have missing values across multiple columns and cannot be imputed accurately, you may choose to remove those observations entirely. \n",
    "        This method can help ensure the quality of your dataset by eliminating potentially unreliable or biased observations. However, it may reduce the sample size and statistical power of your analysis, particularly if missingness is non-random.\n",
    "        <br><br>\n",
    "        <b>6.Imputation before Removal:</b>\n",
    "        <br>\n",
    "        Before removing missing values, you may consider imputing missing values for certain columns to retain as much information as possible. \n",
    "        After imputation, you can then apply one of the aforementioned methods to remove remaining missing values or observations. This approach allows you to balance the trade-off between data completeness and bias reduction.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #66bbf1; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: left; font-size: 22px;\">Here dataset size is small (125 rows), and considering the potential loss of valuable information, we can use the following approaches:</h1>\n",
    "    <p style=\"text-align: left; font-size: 18px;\">1. <strong>Imputation before removal</strong>: Since you have a small dataset, imputing missing values before removing them might be beneficial. You can use appropriate imputation methods (e.g., mean, median, mode imputation for numerical or categorical variables) to fill in missing values, thereby retaining as much information as possible.</p>\n",
    "    <p style=\"text-align: left; font-size: 18px;\">2. <strong>Threshold-based Removal</strong>: Considering the small number of observations, you might set a relatively lenient threshold for removing observations or columns with missing values. This approach allows you to control the extent of data loss while ensuring that the remaining dataset is relatively complete.</p>\n",
    "    <p style=\"text-align: left; font-size: 18px;\">Combining these approaches can help mitigate the loss of information while still addressing the issue of missing values. Here's how you might proceed:</p>\n",
    "    <ul style=\"text-align: left; font-size: 18px;\">\n",
    "        <li><strong>Impute missing values</strong>: Use imputation methods to fill in missing values in your dataset.</li>\n",
    "        <li><strong>Threshold-based removal</strong>: After imputation, apply a threshold-based approach to remove any remaining observations or columns with a high proportion of missing values.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #66bbf1; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: left; font-size: 22px;\">Accesing numeric feature </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021719,
     "end_time": "2024-04-22T18:45:27.379407",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.357688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "df[numeric_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling missing data with mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.044971,
     "end_time": "2024-04-22T18:45:27.434495",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.389524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "for column in numeric_columns:\n",
    "    column_mean = df[column].mean()\n",
    "    df[column].fillna(column_mean, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.020784,
     "end_time": "2024-04-22T18:45:27.465859",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.445075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['type_sports'].unique()\n",
    "df['weight'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.023751,
     "end_time": "2024-04-22T18:45:27.500198",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.476447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['type_sports'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #66bbf1; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: left; font-size: 22px;\">Accesing object type feature </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.037916,
     "end_time": "2024-04-22T18:45:27.549352",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.511436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "df[object_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021792,
     "end_time": "2024-04-22T18:45:27.582778",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.560986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['GPA'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027775,
     "end_time": "2024-04-22T18:45:27.622071",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.594296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Convert non-numeric values to NaN\n",
    "df['GPA'] = pd.to_numeric(df['GPA'], errors='coerce')\n",
    "# Step 2: Calculate the mean value\n",
    "mean_GPA = df['GPA'].mean()\n",
    "# Step 3: Replace NaN values with the mean value\n",
    "df['GPA'].fillna(mean_GPA, inplace=True)\n",
    "# Print the unique values to verify the changes\n",
    "df['GPA'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.022338,
     "end_time": "2024-04-22T18:45:27.656415",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.634077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['weight'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025731,
     "end_time": "2024-04-22T18:45:27.694058",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.668327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n",
    "mean_weight = df['weight'].mean()\n",
    "df['weight'].fillna(mean_weight, inplace=True)\n",
    "df['weight'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.032235,
     "end_time": "2024-04-22T18:45:27.738668",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.706433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.037874,
     "end_time": "2024-04-22T18:45:27.788985",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.751111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "df[object_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.02371,
     "end_time": "2024-04-22T18:45:27.826073",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.802363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in object_columns:\n",
    "    unique_values_count = df[column].nunique()\n",
    "    print(f\"Column '{column}' has {unique_values_count} unique values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.042651,
     "end_time": "2024-04-22T18:45:27.882169",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.839518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in object_columns:\n",
    "    print(f\"Value counts for column '{column}':\")\n",
    "    print(df[column].value_counts().sort_values(ascending=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013337,
     "end_time": "2024-04-22T18:45:27.909334",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.895997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 'fav_cuisine' has unique value with good frequency so encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.053315,
     "end_time": "2024-04-22T18:45:27.976145",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.92283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cuisine_mapping_numeric = {\n",
    "    'Italian': 1,\n",
    "    'Mexican': 2,\n",
    "    'American': 3,\n",
    "}\n",
    "# Encode the 'fav_cuisine' column with numerical values\n",
    "df['fav_cuisine_encoded'] = df['fav_cuisine'].map(cuisine_mapping_numeric).fillna(0)\n",
    "# Display the DataFrame with the new encoded column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.024739,
     "end_time": "2024-04-22T18:45:28.015351",
     "exception": false,
     "start_time": "2024-04-22T18:45:27.990612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['fav_cuisine_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.024279,
     "end_time": "2024-04-22T18:45:28.05394",
     "exception": false,
     "start_time": "2024-04-22T18:45:28.029661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_columns = []\n",
    "for column in df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df[column]):\n",
    "        numeric_columns.append(column)\n",
    "cleaned_df=df[numeric_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ff0000; padding: 22px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: left; font-size: 25px;\"> Final Numeric dataframe </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.048822,
     "end_time": "2024-04-22T18:45:28.11756",
     "exception": false,
     "start_time": "2024-04-22T18:45:28.068738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.01506,
     "end_time": "2024-04-22T18:45:28.148373",
     "exception": false,
     "start_time": "2024-04-22T18:45:28.133313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cleaned_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Pairwise Correlation Heatmap')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numeric features\n",
    "numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(cleaned_df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for selected features\n",
    "selected_features = ['GPA', 'calories_day', 'exercise', 'weight']\n",
    "for col in selected_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=cleaned_df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of GPA vs. weight\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='GPA', y='weight', data=cleaned_df)\n",
    "plt.title('Scatter Plot of GPA vs. Weight')\n",
    "plt.xlabel('GPA')\n",
    "plt.ylabel('Weight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for categorical features\n",
    "categorical_cols = cleaned_df.select_dtypes(include='int64').columns\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x=col, data=cleaned_df)\n",
    "    plt.title(f'Bar Plot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Analysis\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(cleaned_df['weight'], kde=True)\n",
    "plt.title('Distribution of Weight')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Exercise Habits Analysis\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='exercise', data=cleaned_df)\n",
    "plt.title('Exercise Frequency')\n",
    "plt.xlabel('Exercise Frequency')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Daily Calorie Intake Analysis\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(cleaned_df['calories_day'], kde=True)\n",
    "plt.title('Distribution of Daily Calorie Intake')\n",
    "plt.xlabel('Calories per Day')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #66bbf1; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"text-align: left; font-size: 22px;\">if you like my notebook give upvote , it will encourage me thanks for exploring my notebook.</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1133,
     "sourceId": 2082,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.437,
   "end_time": "2024-04-22T18:45:28.582976",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-22T18:45:23.145976",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
